# -*- coding: utf-8 -*-
"""dashboard

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11dRh2qlP8mSX5_5AvzvhQZcVIb1SxMFd
"""

!pip install streamlit
!pip install pyngrok

import pandas as pd

def load_data(path):
    return pd.read_csv(path)

def clean_data(df):
    df = df.dropna()
    df = df.drop_duplicates()
    return df

!jupyter nbconvert --to script notebook1.ipynb

# streamlit_app.py
import streamlit as st
import pandas as pd

st.set_page_config(page_title="Interactive Data Harvesting & Cleaning", layout="wide")

st.title("ðŸ“Š Interactive Data Harvesting & Cleaning (Notebook 1)")

# === Step 1: Data Input ===
st.sidebar.header("Upload or Harvest Data")

uploaded_file = st.sidebar.file_uploader("/content/all_people.csv", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success("âœ… Data loaded successfully!")

    # Preview raw data
    st.subheader("ðŸ”Ž Raw Data Preview")
    st.dataframe(df.head(20))

    # === Step 2: Cleaning Options ===
    st.sidebar.header("Cleaning Options")

    # Drop duplicates
    drop_dupes = st.sidebar.checkbox("Remove duplicates", value=True)

    # Handle missing values
    missing_action = st.sidebar.radio(
        "Handle missing values",
        ["Do nothing", "Drop rows", "Fill with placeholder"],
        index=0
    )

    # Normalize text
    normalize_text = st.sidebar.checkbox("Normalize text columns (lowercase & strip)")

    # Interactive exclusion
    exclude_rows = st.sidebar.checkbox("Add interactive row exclusion column")

    # === Step 3: Apply Cleaning ===
    df_clean = df.copy()

    if drop_dupes:
        df_clean = df_clean.drop_duplicates()

    if missing_action == "Drop rows":
        df_clean = df_clean.dropna()
    elif missing_action == "Fill with placeholder":
        df_clean = df_clean.fillna("MISSING")

    if normalize_text:
        for col in df_clean.select_dtypes(include="object"):
            df_clean[col] = df_clean[col].astype(str).str.strip().str.lower()

    if exclude_rows:
        df_clean["Exclude"] = False
        st.write("You can edit this 'Exclude' column in Excel/Google Sheets after export.")

    # === Step 4: Show Cleaned Data ===
    st.subheader("âœ¨ Cleaned Data Preview")
    st.dataframe(df_clean.head(20))

    # Dataset shape
    st.write(f"Original shape: {df.shape} â†’ Cleaned shape: {df_clean.shape}")

    # === Step 5: Download Cleaned File ===
    st.download_button(
        "ðŸ’¾ Download Cleaned CSV",
        df_clean.to_csv(index=False).encode("utf-8"),
        "cleaned_data.csv",
        "text/csv"
    )
else:
    st.info("Please upload a CSV file to start.")

